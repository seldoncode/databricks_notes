{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4df89f4a-1133-46bc-addf-14eae3f37305",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# **Curso de Databricks: De Cero a Lakehouse**\n",
    "\n",
    "## **√çndice de Contenidos**\n",
    "\n",
    "### **M√≥dulo 1: Configuraci√≥n y Entorno Real**\n",
    "\n",
    "1. **Tema 1: Introducci√≥n a Databricks**\n",
    "* 1.1 ¬øQu√© es Databricks? La cocina industrial de datos\n",
    "* 1.2 El ecosistema: Spark, Delta Lake y MLflow\n",
    "\n",
    "\n",
    "2. **Tema 2: Primeros Pasos Pr√°cticos**\n",
    "* 2.1 Creaci√≥n de cuenta en Databricks Free Edition\n",
    "* 2.2 Interfaz: Workspace, Compute y Unity Catalog\n",
    "* 2.3 Primer vistazo a la jerarqu√≠a: Catalog > Schema > Table\n",
    "* 2.4 Cuidados del entorno gratuito (Auto-termination)\n",
    "* 2.5 Respaldo y Control de Versiones (GitHub)\n",
    "\n",
    "\n",
    "\n",
    "### **M√≥dulo 2: Fundamentos de Procesamiento**\n",
    "\n",
    "3. **Tema 3: Introducci√≥n a los Notebooks**\n",
    "* 3.1 Celdas de c√≥digo y Markdown\n",
    "* 3.2 Comandos m√°gicos (`%python`, `%sql`, `%md`, `%fs`)\n",
    "* 3.3 Trabajo colaborativo y comentarios\n",
    "\n",
    "\n",
    "4. **Tema 4: El motor de Databricks: Apache Spark**\n",
    "* 4.1 El Cerebro y el M√∫sculo: Arquitectura Spark (Driver & Workers).\n",
    "* 4.2 DataFrames: La estructura reina y el concepto de inmutabilidad.\n",
    "* 4.3 El arte de ser \"vago\": Lazy Evaluation (Transformaciones vs. Acciones).\n",
    "* 4.4 Spark UI: Mirando bajo el cap√≥ de tus procesos.\n",
    "\n",
    "\n",
    "5. **Tema 5: Unity Catalog - La Gobernanza Moderna**\n",
    "* 5.1 Del caos al orden: Catalog ‚Üí Schema ‚Üí Table\n",
    "* 5.2 Permisos y lineage de datos\n",
    "* 5.3 Volumes vs DBFS (cu√°ndo usar cada uno)\n",
    "\n",
    "\n",
    "\n",
    "### **M√≥dulo 3: Ingenier√≠a de Datos (ETL)**\n",
    "\n",
    "6. **Tema 6: Almacenamiento con Volumes**\n",
    "* 6.1 Unity Catalog Volumes: El nuevo est√°ndar\n",
    "* 6.2 Creaci√≥n de tu primer Volume\n",
    "* 6.3 Carga de datos desde Volumes\n",
    "* 6.4 Tablas en Unity Catalog (managed vs external)\n",
    "* 6.5 DBFS legacy: cu√°ndo a√∫n lo ve√°s\n",
    "\n",
    "\n",
    "7. **Tema 7: Transformaci√≥n con PySpark**\n",
    "* 7.1 Limpieza: Filtros, nulos y duplicados\n",
    "* 7.2 Manipulaci√≥n de columnas y tipos de datos\n",
    "* 7.3 Agregaciones y Joins complejos\n",
    "\n",
    "\n",
    "8. **Tema 8: Delta Lake: El Coraz√≥n del Lakehouse**\n",
    "* 8.1 ¬øPor qu√© Delta Lake? (Transacciones ACID)\n",
    "* 8.2 Time Travel: Viajando al pasado de tus datos\n",
    "* 8.3 Evoluci√≥n de esquemas y optimizaci√≥n (`Z-ORDER`)\n",
    "\n",
    "\n",
    "\n",
    "### **M√≥dulo 4: An√°lisis y Visualizaci√≥n**\n",
    "\n",
    "9. **Tema 9: Databricks SQL**\n",
    "* 9.1 SQL Nativo en Notebooks\n",
    "* 9.2 Vistas temporales vs Tablas permanentes\n",
    "* 9.3 Funciones de ventana (Window Functions)\n",
    "\n",
    "\n",
    "10. **Tema 10: Visualizaci√≥n de Datos**\n",
    "* 10.1 Gr√°ficos nativos de Databricks\n",
    "* 10.2 Creaci√≥n de Dashboards r√°pidos\n",
    "* 10.3 Integraci√≥n con Power BI/Tableau (Conceptos)\n",
    "\n",
    "\n",
    "\n",
    "### **M√≥dulo 5: Automatizaci√≥n y Machine Learning**\n",
    "\n",
    "11. **Tema 11: Workflows y Orquestaci√≥n**\n",
    "* 11.1 Creaci√≥n de Jobs (Tareas programadas)\n",
    "* 11.2 Encadenamiento de notebooks\n",
    "* 11.3 Notificaciones y control de errores\n",
    "\n",
    "\n",
    "12. **Tema 12: Introducci√≥n a MLflow**\n",
    "* 12.1 El ciclo de vida de un modelo de ML\n",
    "* 12.2 Registro de experimentos y m√©tricas\n",
    "\n",
    "\n",
    "13. **Tema 13: Colaboraci√≥n y Entrega Profesional (CI/CD)**\n",
    "* 13.1 Integraci√≥n con Git (Repos)\n",
    "* 13.2 Estructura de carpetas profesional\n",
    "\n",
    "\n",
    "\n",
    "### **M√≥dulo 6: Cierre de Curso**\n",
    "\n",
    "14. **Tema 14: Proyecto Final Integrador**\n",
    "15. **Tema 15: Pr√≥ximos pasos y Certificaciones**\n",
    "\n",
    "---\n",
    "\n",
    "### **Informaci√≥n General del Curso**\n",
    "\n",
    "* **Duraci√≥n estimada:** 12 semanas.\n",
    "* **Plataforma:** Databricks Free Edition (Community).\n",
    "* **Enfoque:** Ingenier√≠a de Datos y An√°lisis a gran escala.\n",
    "* **Nivel:** Principiante / Intermedio.\n",
    "\n",
    "### **¬øQu√© aprender√°s?**\n",
    "\n",
    "Este curso te llevar√° desde la configuraci√≥n inicial hasta la creaci√≥n de pipelines de datos automatizados. Al finalizar, ser√°s capaz de:\n",
    "\n",
    "* Dominar la interfaz profesional de Databricks.\n",
    "* Procesar millones de filas de datos con **PySpark** y **SQL**.\n",
    "* Implementar la arquitectura **Lakehouse** usando **Delta Lake**.\n",
    "* Mantener tu c√≥digo seguro y versionado en **GitHub**.\n",
    "\n",
    "### **Metodolog√≠a: \"Learn by Doing\"**\n",
    "\n",
    "Este curso est√° dise√±ado para que no seas un espectador, sino un protagonista:\n",
    "\n",
    "* **Teor√≠a m√≠nima ejecutable:** No hay diapositivas; la teor√≠a est√° en celdas Markdown junto al c√≥digo.\n",
    "* **Pr√°ctica inmediata:** Cada concepto se valida con una celda de c√≥digo que t√∫ mismo ejecutar√°s.\n",
    "* **Entorno real:** Usar√°s las mismas herramientas que usan las empresas del Fortune 500.\n",
    "\n",
    "### **Requisitos Previos**\n",
    "\n",
    "1. **L√≥gica de programaci√≥n:** Conocimientos b√°sicos (variables, tipos de datos, bucles).\n",
    "2. **SQL fundamental:** Entender `SELECT`, `FROM`, `WHERE` y `JOIN`.\n",
    "3. **Cuenta de GitHub:** Necesaria para el respaldo de tus apuntes.\n",
    "4. **Curiosidad**: Ganas de romper cosas (en el entorno gratuito, ¬°no cuesta dinero!).\n",
    "\n",
    "### **Estructura de los Apuntes (Leyenda)**\n",
    "\n",
    "Para ayudarte a navegar, ver√°s estos iconos a lo largo del curso:\n",
    "\n",
    "* ‚ö†Ô∏è **Cuidado:** Advertencias para evitar errores comunes o p√©rdida de datos.\n",
    "* üí° **Tip:** Trucos de productividad y atajos.\n",
    "* üöÄ **Reto:** Ejercicios pr√°cticos para poner a prueba lo aprendido.\n",
    "* üìñ **Deep Dive:** Enlaces para profundizar en la documentaci√≥n oficial.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "806a110c-5c9a-44fd-9951-6ab1e27dde38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"¬°Hola desde Databricks!\")\n",
    "print(f\"‚úÖ Spark activo. Versi√≥n: {spark.version}\")\n",
    "print(\"‚úÖ Los Notebooks se guardan autom√°ticamente al ejecutar las celdas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60e3bc01-cb7b-4ec7-a889-64f6dc80f3ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **M√≥dulo 1: Configuraci√≥n y Entorno Real**\n",
    "\n",
    "## **Tema 1: Introducci√≥n a Databricks**\n",
    "\n",
    "### **1.1 ¬øQu√© es Databricks y por qu√© es importante?**\n",
    "\n",
    "> üí° **Bienvenido al entorno de datos del futuro**\n",
    "> Est√°s leyendo esto dentro de un **Notebook de Databricks**. A diferencia de un documento est√°tico, esto es un entorno vivo donde puedes programar, documentar y visualizar resultados en tiempo real.\n",
    "\n",
    "#### **¬øQu√© es Databricks?**\n",
    "\n",
    "Es la plataforma l√≠der de **Data Intelligence** en la nube. Fue creada por los inventores de **Apache Spark** para unificar todo lo que un equipo de datos necesita en un solo lugar.\n",
    "\n",
    "Para entenderlo, olvida los servidores y piensa en comida:\n",
    "\n",
    "| Herramienta | Analog√≠a | Capacidad |\n",
    "| --- | --- | --- |\n",
    "| **Excel / Pandas** | Tu cocina de casa | Ideal para platos peque√±os (datasets que caben en tu RAM local). |\n",
    "| **Databricks** | Una cocina industrial | Dise√±ada para procesar **Petabytes** de datos con eficiencia, orden y muchos cocineros (nodos) trabajando a la vez. |\n",
    "\n",
    "#### **¬øPor qu√© es el est√°ndar de la industria?**\n",
    "\n",
    "1. **Escalabilidad**: Usa el motor **Spark** para dividir tareas pesadas entre varias m√°quinas. Si una tarea tarda 10 horas en tu laptop, Databricks la reparte para que tarde minutos.\n",
    "2. **Colaboraci√≥n**: Es como \"Google Docs para datos\". Varios usuarios pueden editar el mismo c√≥digo a la vez.\n",
    "3. **Arquitectura Lakehouse**: Combina lo barato de un \"Data Lake\" (almac√©n de archivos) con la velocidad y orden de un \"Data Warehouse\" (base de datos profesional).\n",
    "\n",
    "---\n",
    "\n",
    "#### **üíª Prueba de potencia: Pandas vs Spark**\n",
    "\n",
    "Ejecuta la siguiente celda. Vamos a comparar una operaci√≥n tradicional frente al motor distribuido de Databricks.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import time\n",
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "\n",
    "print(\"üìä EXPERIMENTO: Procesando 1 Mill√≥n de filas\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# --- ESCENARIO 1: PANDAS (TRADICIONAL) ---\n",
    "inicio = time.time()\n",
    "df_pd = pd.DataFrame({'id': range(1000000), 'valor': [i * 2 for i in range(1000000)]})\n",
    "resultado_pd = df_pd['valor'].sum()\n",
    "print(f\"‚úÖ Pandas finalizado en: {time.time() - inicio:.4f}s\")\n",
    "\n",
    "# --- ESCENARIO 2: SPARK (DATABRICKS) ---\n",
    "inicio = time.time()\n",
    "df_spark = spark.range(1000000).withColumn(\"valor\", col(\"id\") * 2)\n",
    "resultado_spark = df_spark.select(spark_sum(\"valor\")).collect()[0][0]\n",
    "print(f\"üöÄ Spark finalizado en:  {time.time() - inicio:.4f}s\")\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(\"üí° NOTA: En la Free Edition la diferencia es sutil, pero con 1.000 Millones de filas, Pandas colapsar√≠a y Spark seguir√≠a volando.\")\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **1.2 El ecosistema: Spark, Delta Lake y MLflow**\n",
    "\n",
    "Databricks no es una sola herramienta, es la integraci√≥n perfecta de tres tecnolog√≠as de c√≥digo abierto:\n",
    "\n",
    "1. **Apache Spark (El Motor üèéÔ∏è)**: El encargado del procesamiento distribuido. Divide el problema en trozos y los resuelve en paralelo. Es el \"cerebro\" que ejecuta tu c√≥digo.\n",
    "2. **Delta Lake (El Almac√©n üì¶)**: Es la capa de almacenamiento que da fiabilidad a tus datos. Permite cosas incre√≠bles como el **Time Travel** (consultar c√≥mo estaban tus datos hace una semana) y garantiza que tus tablas no se corrompan si un proceso falla.\n",
    "3. **MLflow (El Laboratorio üß™)**: La herramienta para gestionar modelos de Machine Learning. Registra experimentos, m√©tricas y versiones de modelos para que tus an√°lisis sean reproducibles.\n",
    "\n",
    "---\n",
    "\n",
    "### **Puntos clave para recordar**\n",
    "\n",
    "* **Databricks** unifica ingenier√≠a, anal√≠tica y ciencia de datos.\n",
    "* **Spark** es el motor que permite la escalabilidad.\n",
    "* **Delta Lake** convierte tus archivos en bases de datos robustas.\n",
    "* En este curso, nos enfocaremos principalmente en **Spark** y **Delta Lake**.\n",
    "\n",
    "---\n",
    "\n",
    "**¬øListo para entrar en la cabina de mando?** En el siguiente tema (2.2), daremos un tour por la interfaz para que sepas d√≥nde est√°n los fogones de esta cocina industrial.\n",
    "\n",
    "---\n",
    "\n",
    "**Siguiente paso:** ¬øTe gustar√≠a que preparemos el **Tema 2.2 (Tour por la interfaz)** o prefieres que saltemos a la acci√≥n con el **Tema 3 (Notebooks)** ahora que ya conoces la teor√≠a?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3056056a-3486-4de7-984d-f9d1f1166707",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **Tema 2: Primeros Pasos Pr√°cticos**\n",
    "\n",
    "En este tema vamos a preparar tu entorno de trabajo. Al terminar, tendr√°s un motor encendido, sabr√°s d√≥nde est√°n las herramientas y, lo m√°s importante, tu trabajo estar√° a salvo en la nube.\n",
    "\n",
    "### **2.1 Creaci√≥n de cuenta en Databricks Free Edition**\n",
    "\n",
    "Si est√°s leyendo este notebook, ¬°ya has superado este paso! Solo recuerda: la **Free Edition** (antes Community Edition) es un entorno de aprendizaje gratuito que no requiere tarjeta de cr√©dito y no caduca. Es tu laboratorio personal para experimentar sin miedo a facturas inesperadas.\n",
    "\n",
    "---\n",
    "\n",
    "### **2.2 Interfaz: Workspace, Compute y Unity Catalog**\n",
    "\n",
    "Databricks tiene muchas opciones, pero como principiante, pasar√°s el **99% de tu tiempo** en estos tres iconos de la barra lateral izquierda:\n",
    "\n",
    "| Icono | Nombre | ¬øPara qu√© sirve? | Analog√≠a |\n",
    "| --- | --- | --- | --- |\n",
    "| üìÅ | **Workspace** | Donde guardas tus Notebooks y organizas tus carpetas. | Tu libro de recetas. |\n",
    "| üñ•Ô∏è | **Compute** | Donde creas y gestionas tus Clusters (motores de procesamiento). | Los fogones de la cocina. |\n",
    "| üìñ | **Catalog** | Donde exploras tus bases de datos, tablas y archivos. | La despensa de ingredientes. |\n",
    "\n",
    "> üí° **Tip Profesional**: Usa el buscador de la parte superior (`Ctrl + P`) para encontrar cualquier notebook o tabla r√°pidamente. Es mucho m√°s r√°pido que navegar por las carpetas.\n",
    "\n",
    "\n",
    "> üí° **Nota**: Si ves menciones a \"Hive Metastore\" \n",
    "> en documentaci√≥n antigua, ign√≥ralo. En entornos modernos todo se gestiona \n",
    "> con **Unity Catalog**, que organiza los datos en una jerarqu√≠a de 3 niveles:\n",
    "> **Catalog** (proyecto/departamento) ‚Üí **Schema** (√°rea funcional) ‚Üí \n",
    "> **Table/Volume** (dato concreto).\n",
    "\n",
    "---\n",
    "\n",
    "### **2.3 Primer vistazo a la jerarqu√≠a: Catalog > Schema > Table**\n",
    "\n",
    "Sin un cluster, tu c√≥digo es solo texto. Necesitas \"encender el motor\".\n",
    "\n",
    "**Pasos para un cluster perfecto en Free Edition:**\n",
    "\n",
    "1. Ve a **Compute** -> **Create compute**.\n",
    "2. **Nombre**: El que quieras (ej: `Mi_Motor_Spark`).\n",
    "3. **Databricks Runtime**: Elige siempre la versi√≥n m√°s reciente que diga **LTS** (Long Term Support).\n",
    "4. **Tipo**: En la versi√≥n gratuita ver√°s que es un *Personal Compute* de 15GB. Es m√°s que suficiente.\n",
    "5. Haz clic en **Create**.\n",
    "\n",
    "‚ö†Ô∏è **Paciencia**: Un cluster tarda unos **3-5 minutos** en arrancar. Cuando veas el c√≠rculo verde ‚úÖ, ya puedes conectar tu notebook y empezar a trabajar.\n",
    "\n",
    "---\n",
    "\n",
    "### **2.4 Cuidados del entorno gratuito (Gu√≠a de Supervivencia)**\n",
    "\n",
    "Para no frustrarte, debes conocer las reglas del juego de la versi√≥n gratuita:\n",
    "\n",
    "* **Auto-termination**: El cluster se apaga tras **2 horas de inactividad**. Si el icono est√° gris, simplemente dale a \"Start\" de nuevo.\n",
    "* **Limpieza de Clusters**: Si no usas un cluster en **30 d√≠as**, Databricks lo borrar√° de la lista. **No entres en p√°nico**: tus notebooks (tu c√≥digo) siguen a salvo; solo tienes que crear un cluster nuevo.\n",
    "* **Memoria limitada**: Tienes 15GB. Si intentas cargar un archivo de 50GB, el cluster \"morir√°\" (error *Out of Memory*). Aprenderemos a manejar esto procesando los datos por trozos.\n",
    "\n",
    "---\n",
    "\n",
    "### **2.5 Respaldo y Control de Versiones (GitHub)**\n",
    "\n",
    "No conf√≠es solo en la nube de Databricks; guarda tu progreso en GitHub. Es tu \"seguro de vida\" y tu portafolio profesional.\n",
    "\n",
    "#### **Paso 1: La conexi√≥n**\n",
    "\n",
    "Ve a **Settings** -> **Linked accounts** y configura tu usuario de GitHub con un **Personal Access Token (PAT)**. Esto le da permiso a Databricks para \"escribir\" en tu GitHub.\n",
    "\n",
    "#### **Paso 2: Crear el \"Git Folder\"**\n",
    "\n",
    "1. Ve a **Workspace** -> **Users** -> **[tu-email]**.\n",
    "2. Haz clic derecho -> **Create** -> **Git Folder**.\n",
    "3. Pega la URL de tu repositorio de GitHub (se recomienda que el repo tenga al menos un archivo `README.md` creado).\n",
    "\n",
    "#### **Paso 3: Sincronizar cambios (Commit & Push)**\n",
    "\n",
    "Cuando hagas cambios en tus apuntes y quieras subirlos:\n",
    "\n",
    "1. Haz clic en el nombre de la rama (ej: `main`) arriba a la izquierda.\n",
    "2. **Importante**: En la lista \"Changed files\", **marca la casilla** del archivo que quieres subir. (Si no marcas la casilla, el bot√≥n de mensaje aparecer√° bloqueado üö´).\n",
    "3. Escribe un mensaje descriptivo y pulsa **Commit & Push**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Puntos clave para recordar**\n",
    "\n",
    "* El **Workspace** es para el c√≥digo, el **Catalog** para los datos.\n",
    "* El **Cluster** es temporal, pero tus **Notebooks** son permanentes.\n",
    "* **GitHub** es obligatorio para trabajar como un profesional desde el d√≠a 1.\n",
    "\n",
    "---\n",
    "\n",
    "## **Siguiente paso**\n",
    "\n",
    "Ahora que tenemos la casa en orden, es hora de empezar a escribir. En el **Tema 3: Introducci√≥n a los Notebooks**, aprenderemos los trucos de magia (`%sql`, `%python`, `%md`) para que tus apuntes sean los mejores de la clase.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65594a6a-7a47-49fd-b587-d1254f03fed8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **M√≥dulo 2: Fundamentos de Procesamiento**\n",
    "\n",
    "## **Tema 3: Introducci√≥n a los Notebooks**\n",
    "\n",
    "Un Notebook en Databricks no es un simple archivo de c√≥digo; es un **documento narrativo e interactivo**. Es el lugar donde la ingenier√≠a de datos se encuentra con la documentaci√≥n profesional.\n",
    "\n",
    "---\n",
    "\n",
    "### **3.1 Celdas de c√≥digo y Markdown**\n",
    "\n",
    "El secreto de un buen ingeniero de datos no es solo que su c√≥digo funcione, sino que otros entiendan qu√© hizo. Por eso, los notebooks se dividen en dos tipos de celdas:\n",
    "\n",
    "1. **Celdas de C√≥digo**: Donde ocurre la acci√≥n. Por defecto, tu notebook usar√° el lenguaje que elegiste al crearlo (Python o SQL).\n",
    "2. **Celdas de Markdown**: Donde vive la explicaci√≥n. Usan un lenguaje de etiquetas sencillo para crear t√≠tulos, negritas, listas e incluso insertar im√°genes.\n",
    "\n",
    "#### **üöÄ Atajos de teclado (Para trabajar como un Pro)**\n",
    "\n",
    "Si quieres ir r√°pido, deja de usar el rat√≥n. Estos son los tres comandos que usar√°s mil veces al d√≠a:\n",
    "\n",
    "* **`Shift + Enter`**: Ejecuta la celda actual y salta a la siguiente.\n",
    "* **`Ctrl + Enter`**: Ejecuta la celda actual y se queda en ella.\n",
    "* **`A` (estando fuera de la celda)**: Inserta una celda arriba (*Above*).\n",
    "* **`B` (estando fuera de la celda)**: Inserta una celda abajo (*Below*).\n",
    "\n",
    "---\n",
    "\n",
    "### **3.2 Comandos m√°gicos (Magic Commands)**\n",
    "\n",
    "Databricks es \"pol√≠glota\". Gracias a los comandos m√°gicos, puedes cambiar de lenguaje de programaci√≥n de una celda a otra sin cambiar de archivo. Solo tienes que escribir el comando en la **primera l√≠nea** de la celda.\n",
    "\n",
    "| Comando | Lenguaje / Funci√≥n | Uso principal |\n",
    "| --- | --- | --- |\n",
    "| **`%python`** | Python | Procesamiento de datos con PySpark y l√≥gica compleja. |\n",
    "| **`%sql`** | SQL | Consultas r√°pidas a tablas y creaci√≥n de vistas. |\n",
    "| **`%md`** | Markdown | Documentaci√≥n, explicaciones y t√≠tulos. |\n",
    "| **`%fs`** | File System | Explorar archivos en el DBFS (el disco duro de Databricks). |\n",
    "| **`%sh`** | Shell | Ejecutar comandos de terminal (Linux). |\n",
    "| **`%run`** | Execute | Ejecutar un notebook entero dentro de otro. |\n",
    "\n",
    "> üí° **Tip de experto**: Es muy com√∫n usar Python para limpiar los datos y, en la siguiente celda, usar `%sql` para que el analista de negocio pueda consultar los resultados sin saber programar en Python.\n",
    "\n",
    "---\n",
    "\n",
    "### **3.3 Trabajo colaborativo y comentarios**\n",
    "\n",
    "Databricks naci√≥ para que los equipos de datos dejen de enviarse archivos por email. Es el **\"Google Docs de los datos\"**.\n",
    "\n",
    "* **Edici√≥n en tiempo real**: Puedes ver el cursor de tu compa√±ero movi√©ndose por las celdas mientras trabaj√°is juntos.\n",
    "* **Comentarios (`Ctrl + Alt + M`)**: Si tienes dudas sobre una l√≠nea de c√≥digo, selecci√≥nala y a√±ade un comentario. Puedes mencionar a alguien con `@nombre` para que le llegue una notificaci√≥n.\n",
    "* **Historial de revisiones**: Si borras algo por error o \"rompes\" el c√≥digo, ve al icono de reloj en la barra derecha. Puedes comparar versiones anteriores y restaurar el notebook a un punto en el que funcionaba.\n",
    "\n",
    "---\n",
    "\n",
    "### **Puntos clave para recordar**\n",
    "\n",
    "* Usa **Markdown (`%md`)** para que tus apuntes no sean solo c√≥digo crudo.\n",
    "* Los **Comandos M√°gicos** te permiten usar la mejor herramienta para cada tarea (Python para ETL, SQL para an√°lisis).\n",
    "* El **Historial de Revisiones** es tu red de seguridad antes de hacer un commit a GitHub.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09457466-9449-442e-a49d-8b673affd1d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **Tema 4: El motor de Databricks: Apache Spark**\n",
    "\n",
    "Si Databricks es la cocina, **Apache Spark** es el equipo de chefs de √©lite trabajando en paralelo. En este tema entender√°s por qu√© Spark es capaz de procesar vol√∫menes de datos que har√≠an explotar a un Excel o a un script de Python tradicional.\n",
    "\n",
    "---\n",
    "\n",
    "### **4.1 El Cerebro y el M√∫sculo: Arquitectura Spark (Driver & Workers)**\n",
    "\n",
    "Spark no es un programa que corre en un solo lugar; es un sistema de **computaci√≥n distribuida**. Para que funcione, el trabajo se divide entre dos roles fundamentales:\n",
    "\n",
    "1. **El Driver (El Cerebro) üß†**:\n",
    "* Es el \"Jefe de Cocina\".\n",
    "* Analiza tu c√≥digo, crea un plan de trabajo y decide c√≥mo repartirlo.\n",
    "* Mantiene la comunicaci√≥n con el usuario (tu Notebook).\n",
    "\n",
    "\n",
    "2. **Los Workers (El M√∫sculo) üí™**:\n",
    "* Son los \"Ayudantes de Cocina\".\n",
    "* Ejecutan las tareas que les manda el Driver.\n",
    "* Procesan los datos y devuelven los resultados.\n",
    "\n",
    "\n",
    "\n",
    "> ‚ö†Ô∏è **Nota para el entorno Free**: En la versi√≥n *Community*, tienes un solo nodo que hace de Driver y Worker a la vez. Sin embargo, el c√≥digo que escribes aqu√≠ funcionar√° exactamente igual en un cluster real con **1.000 Workers**.\n",
    "\n",
    "---\n",
    "\n",
    "### **4.2 DataFrames: La estructura reina üëë**\n",
    "\n",
    "En Spark, casi todo lo que hagamos ser√° manipular **DataFrames**.\n",
    "Un DataFrame es una colecci√≥n de datos organizada en columnas con nombre. Piensa en √©l como una tabla de SQL o una hoja de Excel, pero con una diferencia clave: **est√° distribuido**.\n",
    "\n",
    "* **Inmutabilidad**: Una vez creado un DataFrame, no se puede cambiar. Si aplicas un filtro, Spark crea un *nuevo* DataFrame con el resultado. Esto parece ineficiente, pero es lo que permite que Spark sea tan seguro y r√°pido.\n",
    "* **Escalabilidad**: Un DataFrame puede tener trillones de filas. Spark se encarga de que una parte de esas filas est√© en el Worker A y otra parte en el Worker B de forma transparente para ti.\n",
    "\n",
    "---\n",
    "\n",
    "### **4.3 El arte de ser \"vago\": Lazy Evaluation (Transformaciones vs Acciones)**\n",
    "\n",
    "Spark es extremadamente eficiente porque es **vago** (*Lazy*). No hace nada hasta que no tiene m√°s remedio. Para entender esto, dividimos nuestras √≥rdenes en dos tipos:\n",
    "\n",
    "#### **A. Transformaciones (El plan de trabajo)**\n",
    "\n",
    "Son instrucciones como `select()`, `filter()` o `groupBy()`.\n",
    "\n",
    "* **¬øQu√© hace Spark?** Solo toma nota en su libreta. No toca los datos todav√≠a.\n",
    "* **Analog√≠a**: Es como hacer la lista de la compra. Escribir \"comprar leche\" no trae la leche a tu casa.\n",
    "\n",
    "#### **B. Acciones (La ejecuci√≥n)**\n",
    "\n",
    "Son comandos como `show()`, `count()`, `collect()` o `display()`.\n",
    "\n",
    "* **¬øQu√© hace Spark?** Aqu√≠ es donde dice: \"¬°Vale, ahora tengo que trabajar!\". Revisa toda la lista de transformaciones, las optimiza (para ir m√°s r√°pido) y las ejecuta.\n",
    "* **Analog√≠a**: Es el momento de ir al supermercado y comprar todo lo de la lista de una sola vez.\n",
    "\n",
    "---\n",
    "\n",
    "### **4.4 Spark UI: Mirando bajo el cap√≥ üîç**\n",
    "\n",
    "Cuando ejecutas una celda en Databricks, ver√°s un peque√±o enlace debajo que dice **\"Spark Jobs\"**. Si haces clic, se abre la **Spark UI**.\n",
    "\n",
    "* **Jobs y Stages**: Aqu√≠ puedes ver c√≥mo Spark ha dividido tu c√≥digo en peque√±as tareas.\n",
    "* **Optimizaci√≥n**: Puedes ver cu√°nto tiempo ha tardado cada Worker en hacer su parte.\n",
    "* **Depuraci√≥n**: Si una consulta va lenta, la Spark UI te dir√° exactamente en qu√© paso se ha quedado atascado el motor.\n",
    "\n",
    "> üí° **Tip**: No hace falta ser un experto en la Spark UI ahora, pero acost√∫mbrate a abrirla para ver c√≥mo tus √≥rdenes se transforman en tareas paralelas.\n",
    "\n",
    "---\n",
    "\n",
    "### **Puntos clave para recordar**\n",
    "\n",
    "* Spark divide el trabajo entre un **Driver** y m√∫ltiples **Workers**.\n",
    "* Los **DataFrames** son la estructura est√°ndar para manejar datos a escala.\n",
    "* Gracias a la **Lazy Evaluation**, Spark optimiza tus consultas antes de ejecutarlas.\n",
    "* Las **Transformaciones** planifican, las **Acciones** ejecutan.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2bbc5b8-3891-4372-a953-cb2bc9783539",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Curso_Databricks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
