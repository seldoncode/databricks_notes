{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4df89f4a-1133-46bc-addf-14eae3f37305",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **Curso de Databricks**\n",
    "\n",
    "## **√çndice**\n",
    "\n",
    "### **M√≥dulo 1: Configuraci√≥n y Entorno Real**\n",
    "\n",
    "1. **Tema 1: Introducci√≥n a Databricks**\n",
    "* 1.1 ¬øQu√© es Databricks? La cocina industrial de datos\n",
    "* 1.2 El ecosistema: Spark, Delta Lake y MLflow\n",
    "\n",
    "\n",
    "2. **Tema 2: Primeros Pasos Pr√°cticos**\n",
    "* 2.1 Creaci√≥n de cuenta en Databricks Free Edition\n",
    "* 2.2 Interfaz: Workspace, Compute y Cat√°logo\n",
    "* 2.3 Creaci√≥n y gesti√≥n de tu primer Cluster\n",
    "* 2.4 Cuidados del entorno gratuito (Auto-termination)\n",
    "* 2.5 Respaldo y Control de Versiones (GitHub)\n",
    "\n",
    "\n",
    "\n",
    "### **M√≥dulo 2: Fundamentos de Procesamiento**\n",
    "\n",
    "3. **Tema 3: Introducci√≥n a los Notebooks**\n",
    "* 3.1 Celdas de c√≥digo y Markdown\n",
    "* 3.2 Comandos m√°gicos (`%python`, `%sql`, `%md`, `%fs`)\n",
    "* 3.3 Trabajo colaborativo y comentarios\n",
    "\n",
    "\n",
    "4. **Tema 4: El motor de Databricks: Apache Spark**\n",
    "* 4.1 ¬øQu√© es el procesamiento distribuido? (Drivers y Workers)\n",
    "* 4.2 DataFrames: La estructura reina\n",
    "* 4.3 Lazy Evaluation: Transformaciones vs Acciones\n",
    "* 4.4 Spark UI: ¬øQu√© est√° pasando \"bajo el cap√≥\"?\n",
    "\n",
    "\n",
    "\n",
    "### **M√≥dulo 3: Ingenier√≠a de Datos (ETL)**\n",
    "\n",
    "5. **Tema 5: Ingesta y Almacenamiento**\n",
    "* 5.1 DBFS y el sistema de archivos\n",
    "* 5.2 Carga de datos (CSV, JSON, Parquet)\n",
    "* 5.3 Tablas administradas vs Tablas externas\n",
    "\n",
    "\n",
    "6. **Tema 6: Transformaci√≥n con PySpark**\n",
    "* 6.1 Limpieza: Filtros, nulos y duplicados\n",
    "* 6.2 Manipulaci√≥n de columnas y tipos de datos\n",
    "* 6.3 Agregaciones y Joins complejos\n",
    "\n",
    "\n",
    "7. **Tema 7: Delta Lake: El Coraz√≥n del Lakehouse**\n",
    "* 7.1 ¬øPor qu√© Delta Lake? (Transacciones ACID)\n",
    "* 7.2 Time Travel: Viajando al pasado de tus datos\n",
    "* 7.3 Evoluci√≥n de esquemas y optimizaci√≥n (`Z-ORDER`)\n",
    "\n",
    "\n",
    "\n",
    "### **M√≥dulo 4: An√°lisis y Visualizaci√≥n**\n",
    "\n",
    "8. **Tema 8: Databricks SQL**\n",
    "* 8.1 SQL Nativo en Notebooks\n",
    "* 8.2 Vistas temporales vs Tablas permanentes\n",
    "* 8.3 Funciones de ventana (Window Functions)\n",
    "\n",
    "\n",
    "9. **Tema 9: Visualizaci√≥n de Datos**\n",
    "* 9.1 Gr√°ficos nativos de Databricks\n",
    "* 9.2 Creaci√≥n de Dashboards r√°pidos\n",
    "* 9.3 Integraci√≥n con Power BI/Tableau (Conceptos)\n",
    "\n",
    "\n",
    "\n",
    "### **M√≥dulo 5: Automatizaci√≥n y Machine Learning**\n",
    "\n",
    "10. **Tema 10: Workflows y Orquestaci√≥n**\n",
    "* 10.1 Creaci√≥n de Jobs (Tareas programadas)\n",
    "* 10.2 Encadenamiento de notebooks\n",
    "* 10.3 Notificaciones y control de errores\n",
    "\n",
    "\n",
    "11. **Tema 11: Introducci√≥n a MLflow**\n",
    "* 11.1 El ciclo de vida de un modelo de ML\n",
    "* 11.2 Registro de experimentos y m√©tricas\n",
    "\n",
    "\n",
    "12. **Tema 12: Buenas Pr√°cticas y CI/CD**\n",
    "* 12.1 Integraci√≥n con Git (Repos)\n",
    "* 12.2 Estructura de carpetas profesional\n",
    "\n",
    "\n",
    "\n",
    "### **M√≥dulo 6: Cierre de Curso**\n",
    "\n",
    "13. **Tema 13: Proyecto Final Integrador**\n",
    "14. **Tema 14: Pr√≥ximos pasos y Certificaciones**\n",
    "\n",
    "---\n",
    "\n",
    "### **Informaci√≥n del Curso**\n",
    "\n",
    "* **Duraci√≥n estimada:** 8 - 9 semanas (a tu propio ritmo).\n",
    "* **Plataforma principal:** Databricks Free Edition.\n",
    "* **Nivel:** Principiante / Intermedio.\n",
    "\n",
    "### **¬øQu√© ser√°s capaz de hacer?**\n",
    "\n",
    "Al terminar este curso, habr√°s pasado de no conocer la herramienta a ser capaz de:\n",
    "\n",
    "* **Montar un entorno** de Big Data funcional en minutos.\n",
    "* **Construir pipelines de datos (ETL)** robustos usando Delta Lake.\n",
    "* **Alternar fluidamente** entre Python y SQL seg√∫n la necesidad.\n",
    "* **Automatizar procesos** para que tus an√°lisis se ejecuten solos.\n",
    "\n",
    "### **Metodolog√≠a: \"Learn by Doing\"**\n",
    "\n",
    "Este curso huye de las diapositivas infinitas. Cada tema se compone de:\n",
    "\n",
    "* üíª **Notebooks Interactivos**: Explicaci√≥n y c√≥digo en el mismo lugar.\n",
    "* üõ†Ô∏è **Retos Pr√°cticos**: Peque√±os desaf√≠os al final de cada secci√≥n.\n",
    "* üìÇ **Datos Reales**: Trabajaremos con datasets de e-commerce, clima y finanzas.\n",
    "\n",
    "### **Requisitos**\n",
    "\n",
    "1. **L√≥gica de programaci√≥n**: No hace falta ser experto, pero s√≠ entender qu√© es una variable o un bucle.\n",
    "2. **SQL b√°sico**: Saber qu√© es un `SELECT` y un `JOIN`.\n",
    "3. **Curiosidad**: Ganas de romper cosas (en el entorno gratuito, ¬°no cuesta dinero!).\n",
    "\n",
    "### **Leyenda de los Apuntes**\n",
    "\n",
    "Para facilitar la lectura, usaremos estos iconos:\n",
    "\n",
    "* ‚ö†Ô∏è **Cuidado**: Errores comunes o configuraciones que pueden costar tiempo/dinero.\n",
    "* üí° **Pro-Tip**: Trucos de experto para ir m√°s r√°pido.\n",
    "* üöÄ **Reto**: Ejercicio pr√°ctico para validar lo aprendido.\n",
    "* üìñ **Deep Dive**: Enlaces a documentaci√≥n oficial para los m√°s curiosos.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "806a110c-5c9a-44fd-9951-6ab1e27dde38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"¬°Hola desde Databricks!\")\n",
    "print(f\"‚úÖ Spark activo. Versi√≥n: {spark.version}\")\n",
    "print(\"‚úÖ Los Notebooks se guardan autom√°ticamente al ejecutar las celdas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60e3bc01-cb7b-4ec7-a889-64f6dc80f3ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **M√≥dulo 1: Configuraci√≥n y Entorno Real**\n",
    "\n",
    "## **Tema 1: Introducci√≥n a Databricks**\n",
    "\n",
    "### **1.1 ¬øQu√© es Databricks y por qu√© es importante?**\n",
    "\n",
    "> üí° **Bienvenido al entorno de datos del futuro**\n",
    "> Est√°s abriendo estos apuntes dentro de **Databricks**. A diferencia de un documento PDF o una web est√°tica, esto es un **entorno vivo**. Todo lo que leas aqu√≠ puedes probarlo, modificarlo y ejecutarlo.\n",
    "\n",
    "---\n",
    "\n",
    "### **¬øQu√© es Databricks?**\n",
    "\n",
    "En pocas palabras: **Databricks es la plataforma donde los datos se vuelven √∫tiles.**\n",
    "\n",
    "Fue creada por los inventores de **Apache Spark**, con una idea clara: el procesamiento de datos no deber√≠a ser un dolor de cabeza t√©cnico. Es una plataforma en la nube (Azure, AWS o GCP) que unifica todo lo que un equipo de datos necesita.\n",
    "\n",
    "#### **La analog√≠a definitiva: La Cocina Industrial**\n",
    "\n",
    "Para entender Databricks, olvida los servidores y piensa en comida:\n",
    "\n",
    "| Herramienta | Analog√≠a | Capacidad |\n",
    "| --- | --- | --- |\n",
    "| **Excel / Pandas** | Tu cocina de casa | Ideal para cenas peque√±as (datasets que caben en tu RAM). |\n",
    "| **Databricks** | Una cocina industrial | Dise√±ada para servir a miles de personas simult√°neamente (Petabytes de datos) con eficiencia y orden. |\n",
    "\n",
    "---\n",
    "\n",
    "### **¬øPor qu√© es el est√°ndar de la industria?**\n",
    "\n",
    "#### **1. El fin del \"en mi m√°quina funciona\"**\n",
    "\n",
    "Databricks ofrece un **entorno compartido**. T√∫ y tu equipo trabaj√°is sobre el mismo c√≥digo y los mismos datos en tiempo real, como si fuera un Google Docs para programadores.\n",
    "\n",
    "#### **2. Velocidad y Escala (El motor Spark)**\n",
    "\n",
    "Si intentas abrir un archivo de 100GB en tu laptop, probablemente explote. Databricks utiliza **procesamiento distribuido**: divide el problema en trozos peque√±os y los reparte entre varias m√°quinas (un \"Cluster\").\n",
    "\n",
    "#### **3. La arquitectura Lakehouse**\n",
    "\n",
    "Antes, las empresas ten√≠an los datos divididos en dos sitios:\n",
    "\n",
    "1. **Data Lake**: Barato pero desordenado (un \"trastero\" de archivos).\n",
    "2. **Data Warehouse**: Ordenado y r√°pido pero muy caro.\n",
    "\n",
    "**Databricks une ambos en el \"Lakehouse\"**: tienes la flexibilidad y bajo coste de un lago de datos con el orden y la velocidad de un almac√©n profesional.\n",
    "\n",
    "---\n",
    "\n",
    "### **üíª Prueba de fuego: ¬øRealmente hay diferencia?**\n",
    "\n",
    "Ejecuta la siguiente celda. Vamos a comparar c√≥mo \"piensa\" una herramienta tradicional frente al motor de Databricks (Spark).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ec8a6a5-5885-4c30-a572-ad7d04748d97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "\n",
    "# --- ESCENARIO 1: PANDAS (TRADICIONAL) ---\n",
    "# Simula procesar datos en la memoria de una sola m√°quina\n",
    "print(\"üêº Simulando procesamiento con Pandas...\")\n",
    "inicio = time.time()\n",
    "df_pd = pd.DataFrame({'id': range(10_000_000), 'valor': [i * 2 for i in range(10_000_000)]})\n",
    "resultado_pd = df_pd['valor'].sum()\n",
    "print(f\"‚úÖ Finalizado en: {time.time() - inicio:.4f} segundos.\")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- ESCENARIO 2: SPARK (DATABRICKS) ---\n",
    "# Utiliza el motor distribuido (incluso en Free Edition es m√°s eficiente con grandes vol√∫menes)\n",
    "print(\"üöÄ Procesando con el motor Spark de Databricks...\")\n",
    "inicio = time.time()\n",
    "df_spark = spark.range(10_000_000).withColumn(\"valor\", col(\"id\") * 2)\n",
    "resultado_spark = df_spark.select(spark_sum(\"valor\")).collect()[0][0]\n",
    "print(f\"‚úÖ Finalizado en: {time.time() - inicio:.4f} segundos.\")\n",
    "\n",
    "print(\"\\nüí° NOTA: Con 10 millones de filas la diferencia es peque√±a, pero con 1.000 millones, Pandas fallar√≠a y Spark seguir√≠a volando.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fffa2b0a-f706-4096-ad6a-8803999e353d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **‚ö†Ô∏è Sobre la \"Free Edition\" que est√°s usando**\n",
    "\n",
    "Como este es un curso pr√°ctico, usaremos la versi√≥n gratuita. Ten en cuenta estas reglas de oro:\n",
    "\n",
    "* **Tu trabajo est√° a salvo**: Los notebooks (tus apuntes) no se borran nunca.\n",
    "* **El \"Motor\" descansa**: El cluster (el ordenador que procesa el c√≥digo) se apaga tras **2 horas de inactividad**. Si ves un punto gris arriba a la derecha, solo tienes que darle a \"Start\" de nuevo.\n",
    "* **Sin coste**: No necesitas tarjeta de cr√©dito. Es el entorno perfecto para romper cosas y aprender.\n",
    "\n",
    "---\n",
    "\n",
    "### **Puntos clave para recordar**\n",
    "\n",
    "1. **Databricks** es una plataforma unificada (ETL, SQL, ML).\n",
    "2. Utiliza **Spark** para procesar datos de forma masiva y r√°pida.\n",
    "3. Su arquitectura se llama **Lakehouse** (lo mejor de dos mundos).\n",
    "4. Es **colaborativo**: ideal para equipos modernos.\n",
    "\n",
    "---\n",
    "\n",
    "**¬øListo para ensuciarte las manos?** En el siguiente tema (1.2) veremos c√≥mo se conectan las piezas del ecosistema antes de crear tu propio cluster.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a7bc4f8-671d-422e-a111-36c36282517c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **1.2 El ecosistema: Spark, Delta Lake y MLflow**\n",
    "\n",
    "Databricks es tan potente porque no intenta reinventar la rueda. En su lugar, toma las tres tecnolog√≠as de c√≥digo abierto m√°s importantes del mundo de los datos y las integra a la perfecci√≥n.\n",
    "\n",
    "Para entender el ecosistema, imagina que Databricks es el **sistema operativo** y estas tres herramientas son sus aplicaciones principales:\n",
    "\n",
    "#### **1. Apache Spark: El Motor üèéÔ∏è**\n",
    "\n",
    "Es el encargado de hacer el trabajo sucio. Spark es un motor de procesamiento distribuido.\n",
    "\n",
    "* **¬øPara qu√© sirve?** Para transformar, filtrar y agrupar datos masivos en segundos.\n",
    "* **Concepto clave:** Spark no usa un solo procesador; reparte la tarea entre muchos. Si una tarea tarda 10 horas en un ordenador, Spark la divide en 10 ordenadores para que tarde 1 hora.\n",
    "\n",
    "#### **2. Delta Lake: El Almac√©n Inteligente üì¶**\n",
    "\n",
    "Es la tecnolog√≠a que convierte un \"lago de datos\" (desordenado) en una base de datos fiable.\n",
    "\n",
    "* **¬øPara qu√© sirve?** Para que tus archivos CSV o Parquet se comporten como tablas de una base de datos profesional.\n",
    "* **Concepto clave:** **Transacciones ACID**. Si un proceso de escritura falla a mitad, Delta Lake se asegura de que no queden datos corruptos. O se escribe todo, o no se escribe nada. Adem√°s, permite el \"Time Travel\" (volver a versiones anteriores de los datos).\n",
    "\n",
    "#### **3. MLflow: El Laboratorio de Experimentos üß™**\n",
    "\n",
    "Es la herramienta para gestionar el ciclo de vida del Machine Learning.\n",
    "\n",
    "* **¬øPara qu√© sirve?** Para llevar un diario perfecto de tus modelos. Registra qu√© datos usaste, qu√© par√°metros elegiste y qu√© resultado obtuviste.\n",
    "* **Concepto clave:** **Reproducibilidad**. Te permite repetir un experimento meses despu√©s y obtener exactamente el mismo resultado.\n",
    "\n",
    "---\n",
    "\n",
    "### **üí° ¬øC√≥mo trabajan juntos? (El flujo real)**\n",
    "\n",
    "Imagina que trabajas en una empresa de transporte:\n",
    "\n",
    "1. **Spark** lee millones de registros de GPS en tiempo real.\n",
    "2. **Delta Lake** guarda esos datos de forma segura, permiti√©ndote consultar la ubicaci√≥n de un cami√≥n hace 3 d√≠as (Time Travel).\n",
    "3. **MLflow** entrena un modelo para predecir a qu√© hora llegar√° el cami√≥n a su destino bas√°ndose en el hist√≥rico guardado en Delta.\n",
    "\n",
    "---\n",
    "\n",
    "### **‚ö†Ô∏è Nota sobre el enfoque de este curso**\n",
    "\n",
    "Aunque Databricks incluye las tres, este curso se centrar√° un **70% en Spark** (procesamiento) y un **25% en Delta Lake** (almacenamiento), ya que son los cimientos de cualquier Ingeniero o Analista de Datos. Veremos **MLflow** al final para entender c√≥mo se cierra el c√≠rculo.\n",
    "\n",
    "---\n",
    "\n",
    "### **Puntos clave para recordar**\n",
    "\n",
    "* **Spark** = Computaci√≥n (Cerebro).\n",
    "* **Delta Lake** = Almacenamiento fiable (Memoria).\n",
    "* **MLflow** = Gesti√≥n de modelos (Diario de experimentos).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cd8e2bd-93df-4547-a856-31f45db1d39c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **2.2 Tour por la interfaz: Workspace, Compute y Catalog**\n",
    "\n",
    "Ya tienes las llaves de la \"cocina industrial\", ahora vamos a ver d√≥nde est√°n los fogones, la despensa y el libro de recetas. Aunque Databricks tiene muchas opciones, para este curso solo necesitas dominar tres iconos de la barra lateral izquierda.\n",
    "\n",
    "#### **1. Workspace (El Libro de Recetas) üìù**\n",
    "\n",
    "Es tu explorador de archivos, pero orientado a la colaboraci√≥n.\n",
    "\n",
    "* **¬øQu√© hay aqu√≠?** Tus notebooks (los archivos de c√≥digo), carpetas compartidas y archivos de configuraci√≥n.\n",
    "* **Pro-Tip:** Aqu√≠ es donde crear√°s una carpeta con tu nombre para organizar los ejercicios de este curso.\n",
    "* **En la Free Edition:** Tienes una carpeta llamada \"Users/tu-email\" que es privada para ti.\n",
    "\n",
    "#### **2. Compute (Los Fogones) ‚öôÔ∏è**\n",
    "\n",
    "Aqu√≠ es donde ocurre la magia. Sin \"Compute\", tu c√≥digo es solo texto muerto.\n",
    "\n",
    "* **¬øQu√© hay aqu√≠?** Los **Clusters**. Un cluster es un grupo de ordenadores (o uno solo en la versi√≥n gratuita) que ejecutan el c√≥digo Spark.\n",
    "* **Importante:** Antes de ejecutar cualquier celda, debes asegurarte de que tienes un cluster encendido y que tu notebook est√° \"conectado\" a √©l.\n",
    "* **Regla de oro:** Si el icono del cluster est√° en **gris**, est√° apagado. Si est√° en **verde**, ¬°puedes cocinar!\n",
    "\n",
    "#### **3. Catalog / Data (La Despensa) üóÑÔ∏è**\n",
    "\n",
    "Aqu√≠ es donde gestionas tus datos.\n",
    "\n",
    "* **¬øQu√© hay aqu√≠?** Tus bases de datos, tablas y el acceso al sistema de archivos (DBFS).\n",
    "* **Concepto clave:** Desde aqu√≠ podr√°s ver qu√© columnas tiene una tabla, una muestra de los datos y qui√©n tiene permiso para verla.\n",
    "\n",
    "---\n",
    "\n",
    "### **üõ†Ô∏è Gu√≠a r√°pida de navegaci√≥n**\n",
    "\n",
    "| Si quieres... | Ve a... | Atajo de teclado |\n",
    "| --- | --- | --- |\n",
    "| **Escribir c√≥digo** | Workspace | `Alt + Shift + W` |\n",
    "| **Ver si el motor est√° encendido** | Compute | - |\n",
    "| **Buscar una tabla espec√≠fica** | Catalog | `Alt + Shift + D` |\n",
    "| **Buscar cualquier cosa** | Buscador (arriba) | `Ctrl + P` |\n",
    "\n",
    "---\n",
    "\n",
    "### **‚ö†Ô∏è Diferencia Crucial: ¬øD√≥nde est√°n mis archivos?**\n",
    "\n",
    "Es com√∫n confundir el **Workspace** con el **Catalog**:\n",
    "\n",
    "* En el **Workspace** guardas tu **c√≥digo** (el \"c√≥mo\" se hace).\n",
    "* En el **Catalog** gestionas tus **datos** (el \"qu√©\" se procesa).\n",
    "\n",
    "---\n",
    "\n",
    "### **Puntos clave para recordar**\n",
    "\n",
    "1. El **Workspace** es para organizar tus notebooks.\n",
    "2. El **Compute** es obligatorio para que el c√≥digo funcione (es tu motor).\n",
    "3. El **Catalog** es el mapa de tus bases de datos y tablas.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "586ed048-12c2-4915-9b35-9cb08df5c5bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **2.3 Creaci√≥n y gesti√≥n de tu primer Cluster**\n",
    "\n",
    "Un **Cluster** es el motor de tu coche. Sin √©l, puedes ver el volante y los asientos (el Workspace), pero no vas a ning√∫n lado. En la versi√≥n gratuita, Databricks nos regala un peque√±o motor de **15 GB de memoria**, m√°s que suficiente para aprender.\n",
    "\n",
    "#### **Pasos para crear tu primer cluster**\n",
    "\n",
    "1. En la barra lateral, haz clic en **Compute**.\n",
    "2. Haz clic en el bot√≥n azul **Create compute**.\n",
    "3. **Nombre del cluster:** Dale un nombre √©pico (ej: `Mi_Primer_Cluster` o `Motor_Spark`).\n",
    "4. **Databricks Runtime Version:** Esta es la versi√≥n del sistema operativo y de Spark.\n",
    "* *Recomendaci√≥n:* Elige siempre la versi√≥n m√°s reciente que diga **LTS** (Long Term Support). Esto asegura que es estable.\n",
    "\n",
    "\n",
    "5. Haz clic en **Create Cluster**.\n",
    "\n",
    "> ‚è≥ **Paciencia de principiante**: Un cluster tarda entre **3 y 5 minutos** en arrancar. Ver√°s un c√≠rculo dando vueltas. Cuando cambie a un **check verde** ‚úÖ, ya puedes ejecutar c√≥digo.\n",
    "\n",
    "---\n",
    "\n",
    "### **‚ö†Ô∏è Reglas de supervivencia en la Free Edition**\n",
    "\n",
    "Para que no te lleves sorpresas, memoriza estas tres cosas sobre tu cluster gratuito:\n",
    "\n",
    "* **Autotermination**: Si dejas de usar el notebook durante **2 horas**, Databricks apagar√° el cluster autom√°ticamente para ahorrar recursos. Es normal. Solo tendr√°s que volver a \"Compute\" y darle al Play (Start).\n",
    "* **Recursos compartidos**: Tienes 15GB de RAM. Si intentas procesar un archivo de 50GB de golpe, el cluster \"morir√°\" (ver√°s un error de *Out of Memory*). Aprenderemos a manejar esto m√°s adelante.\n",
    "* **Borrado tras 30 d√≠as**: Si no usas tu cluster en 30 d√≠as, Databricks lo eliminar√° de la lista. **¬°No entres en p√°nico!** Tus notebooks y datos no se borran; solo tienes que crear un cluster nuevo y volver a conectar tus notebooks.\n",
    "\n",
    "---\n",
    "\n",
    "### **üîó C√≥mo conectar un Notebook a tu Cluster**\n",
    "\n",
    "Una vez que el cluster est√© verde, ve a tu Notebook y mira en la esquina superior derecha.\n",
    "\n",
    "1. Haz clic en **Connect** (o en el desplegable de clusters).\n",
    "2. Selecciona el cluster que acabas de crear.\n",
    "3. Ahora, al pulsar `Shift + Enter` en una celda, el c√≥digo se enviar√° a ese motor para ser ejecutado.\n",
    "\n",
    "---\n",
    "\n",
    "### **Puntos clave para recordar**\n",
    "\n",
    "1. El cluster es el **recurso de c√≥mputo** necesario para ejecutar Spark.\n",
    "2. En la Free Edition es de **un solo nodo** (15GB RAM).\n",
    "3. Se apaga tras **2 horas** de inactividad.\n",
    "4. Debes **conectar** manualmente el notebook al cluster antes de trabajar.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f915c3d9-70b4-49ea-bbfd-85763ca10f6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **2.4 Cuidados del entorno gratuito (Gu√≠a de supervivencia)**\n",
    "\n",
    "Para que tu experiencia con la Free Edition sea fluida y no pierdas tiempo reiniciando cosas, sigue estos consejos:\n",
    "\n",
    "1. **El Cluster es \"ef√≠mero\"**: Si el punto junto al nombre de tu cluster est√° gris, tus variables y datos cargados en memoria se han borrado. Tendr√°s que ejecutar las celdas de nuevo (puedes usar `Run All`).\n",
    "2. **No satures los 15GB**: La Free Edition solo tiene un nodo. Si intentas cargar un CSV de 20GB, el cluster se detendr√°.\n",
    "* *Truco*: Usa `display(df.limit(1000))` para previsualizar datos en lugar de intentar ver millones de filas.\n",
    "\n",
    "\n",
    "3. **DBFS es tu disco duro**: Los archivos que subas a la carpeta `/FileStore` o al sistema de archivos de Databricks (DBFS) **S√ç** se mantienen aunque el cluster se apague.\n",
    "4. **L√≠mite de 2 horas**: Si vas a irte a comer, guarda tus cambios. Al volver, el cluster estar√° apagado. Solo dale a **\"Start\"** y espera 3 minutos.\n",
    "\n",
    "---\n",
    "\n",
    "### **2.5 Respaldo y Control de Versiones con GitHub**\n",
    "\n",
    "En el mundo profesional, nadie guarda el c√≥digo solo en Databricks. Usamos **Git** para que nuestros apuntes y scripts vivan para siempre en la nube (y para poder volver atr√°s si borramos algo por error).\n",
    "\n",
    "#### **Paso 1: Conectar Databricks con tu GitHub**\n",
    "\n",
    "1. En Databricks, ve a tu **User Settings** (el icono de perfil arriba a la derecha) -> **Settings**.\n",
    "2. Busca la pesta√±a **Linked accounts** (o Git Integration).\n",
    "3. Selecciona **GitHub** y a√±ade tu nombre de usuario.\n",
    "4. Necesitar√°s un **Personal Access Token (PAT)** de GitHub.\n",
    "* *C√≥mo sacarlo*: En GitHub: Settings -> Developer Settings -> Personal Access Tokens -> Tokens (classic) -> Generate new token (con permisos de `repo`).\n",
    "5. Pega el token en Databricks y guarda.\n",
    "\n",
    "#### **Paso 2: Crear un \"Git Folder\" en Databricks**\n",
    "\n",
    "1. En la barra lateral, ve a **Workspace**.\n",
    "2. Busca la carpeta **Users** y localiza la que tiene tu **correo electr√≥nico**.\n",
    "3. Haz clic derecho sobre ella -> **Create** -> **Git Folder**.\n",
    "3. En el cuadro que aparece, pega la URL del repositorio que hayas creado en GitHub (se recomienda que al crearlo en GitHub marques la casilla \"Add a README file\").\n",
    "4. ¬°Listo! Ahora tus notebooks est√°n dentro de una carpeta sincronizada con Git.\n",
    "\n",
    "#### **Paso 3: Hacer \"Commit\" y \"Push\"**\n",
    "\n",
    "Cuando termines tus apuntes del d√≠a:\n",
    "\n",
    "1. Haz clic en el nombre de la **Rama** (arriba a la izquierda del notebook, dir√° `main`).\n",
    "2. Escribe un mensaje de lo que has hecho (ej: \"Apuntes del M√≥dulo 2 finalizados\").\n",
    "3. Dale al bot√≥n **Commit & Push**. Tus apuntes ya est√°n seguros en tu cuenta de GitHub.\n",
    "\n",
    "> üí° **Plan B (R√°pido)**: Si no quieres configurar Git todav√≠a, puedes ir a **File -> Export -> Dashboards/Source File** y descargar tu notebook como un archivo `.ipynb` (compatible con Jupyter) o `.dbc` (formato Databricks) en tu ordenador.\n",
    "\n",
    "---\n",
    "\n",
    "### **Puntos clave para recordar**\n",
    "\n",
    "* **2.4:** El cluster es temporal, pero los archivos en DBFS y los notebooks son permanentes.\n",
    "* **2.5:** GitHub es el \"seguro de vida\" de tu c√≥digo. Aprende a hacer *Commit* al final de cada sesi√≥n.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3056056a-3486-4de7-984d-f9d1f1166707",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Curso_Databricks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
